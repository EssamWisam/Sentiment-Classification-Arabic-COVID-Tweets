{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextual_preprocess(tweet):\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet = re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)', '', tweet)\n",
    "    tweet = re.sub(r'[\\u064B-\\u0652]', '', tweet)\n",
    "    tweet = re.sub(r'[^\\u0621-\\u064A\\u0660-\\u0669 ]+', ' ', tweet)\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextual_embeddings(tweets):\n",
    "  base_model_name = 'moha/arabert_c19'\n",
    "  tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "  base_model = AutoModel.from_pretrained(base_model_name).to(device)\n",
    "  base_model.eval()\n",
    "  for param in base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "  tokens = [tokenizer(tweet,\n",
    "                padding='max_length',\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\") for tweet in tweets\n",
    "  ]\n",
    "\n",
    "  embeddings = [(base_model(input_ids=token['input_ids'].to(device),\n",
    "                            attention_mask=token['attention_mask'].to(device),\n",
    "                            return_dict=False)[1]).detach().cpu()\n",
    "                for token in tqdm(tokens)]\n",
    "\n",
    "  return embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the Contextual Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../Dataset/SavedFeatures/CONTEXT/x.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/essam/Documents/GitHub/NLP-Project/2-FeatureExtraction/ContextualEmbeddings/ContextualEmbeddings.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/essam/Documents/GitHub/NLP-Project/2-FeatureExtraction/ContextualEmbeddings/ContextualEmbeddings.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m saved \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/essam/Documents/GitHub/NLP-Project/2-FeatureExtraction/ContextualEmbeddings/ContextualEmbeddings.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mif\u001b[39;00m saved:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/essam/Documents/GitHub/NLP-Project/2-FeatureExtraction/ContextualEmbeddings/ContextualEmbeddings.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(DIR(\u001b[39m'\u001b[39;49m\u001b[39mx\u001b[39;49m\u001b[39m'\u001b[39;49m), allow_pickle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/essam/Documents/GitHub/NLP-Project/2-FeatureExtraction/ContextualEmbeddings/ContextualEmbeddings.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(DIR(\u001b[39m'\u001b[39m\u001b[39mX_test\u001b[39m\u001b[39m'\u001b[39m), allow_pickle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/essam/Documents/GitHub/NLP-Project/2-FeatureExtraction/ContextualEmbeddings/ContextualEmbeddings.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/npyio.py:407\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    405\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 407\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    408\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../Dataset/SavedFeatures/CONTEXT/x.npy'"
     ]
    }
   ],
   "source": [
    "DIR = lambda x : f'../../Dataset/SavedFeatures/CONTEXT/{x}.npy'\n",
    "D_DIR = lambda x : f'../../Dataset/{x}.npy'\n",
    "\n",
    "Preprocessing = {\"approach\": \"contextual\"}\n",
    "Features = {\"approach\": \"contextual\"}\n",
    "\n",
    "y1 = np.load(D_DIR('y1'), allow_pickle=True)\n",
    "y2 = np.load(D_DIR('y2'), allow_pickle=True)\n",
    "Y1 = np.load(D_DIR('Y1_test'), allow_pickle=True)\n",
    "Y2 = np.load(D_DIR('Y2_test'), allow_pickle=True)\n",
    "\n",
    "saved = True\n",
    "if saved:\n",
    "    x = np.load(DIR('x'), allow_pickle=True)\n",
    "    X = np.load(DIR('X_test'), allow_pickle=True)\n",
    "    \n",
    "else:\n",
    "   x = pd.read_csv('../../Dataset/train.csv')\n",
    "   X = pd.read_csv('../../Dataset/dev.csv')\n",
    "   x = [contextual_preprocess(tweet) for tweet in x['text']]\n",
    "   X = [contextual_preprocess(tweet) for tweet in X['text']]\n",
    "   x = contextual_embeddings(x)\n",
    "   X = contextual_embeddings(X)\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
